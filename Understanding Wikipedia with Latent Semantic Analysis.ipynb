{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from file:/home/jovyan/aas/ch06-lsa/target/ch06-lsa-2.0.0-jar-with-dependencies.jar\n",
      "Finished download of ch06-lsa-2.0.0-jar-with-dependencies.jar\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:/home/jovyan/aas/ch06-lsa/target/ch06-lsa-2.0.0-jar-with-dependencies.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@32ae1ce3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "i...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://74107bd78b99:4040)\" target=\"new_tab\">Spark UI: local-1539613677795</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1539613677795: Some(http://74107bd78b99:4040)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import edu.umd.cloud9.collection.XMLInputFormat\n",
    "import edu.stanford.nlp.ling.CoreAnnotations.{LemmaAnnotation, SentencesAnnotation, TokensAnnotation}\n",
    "import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}\n",
    "import edu.umd.cloud9.collection.wikipedia.WikipediaPage\n",
    "import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\n",
    "import java.util.Properties\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io.{LongWritable, Text}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, IDF}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path = Data/Wikipedia-Geometry.xml\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "kvs = Data/Wikipedia-Geometry.xml NewHadoopRDD[4] at newAPIHadoopFile at <console>:66\n",
       "rawXmls = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"Data/Wikipedia-Geometry.xml\"\n",
    "@transient val conf = new Configuration()\n",
    "conf.set(XMLInputFormat.START_TAG_KEY, \"<page>\")\n",
    "conf.set(XMLInputFormat.END_TAG_KEY, \"</page>\")\n",
    "val kvs = spark.sparkContext.newAPIHadoopFile(path, classOf[XMLInputFormat],\n",
    "                                              classOf[LongWritable], classOf[Text], conf)\n",
    "val rawXmls = kvs.map(_._2.toString).toDS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "|<page>\n",
      "    <title...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawXmls.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docTexts = [_1: string, _2: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "wikiXmlToPlainText: (pageXml: String)Option[(String, String)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_1: string, _2: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wikiXmlToPlainText(pageXml: String): Option[(String, String)] = {\n",
    "// Wikipedia has updated their dumps slightly since Cloud9 was written,\n",
    "// so this hacky replacement is sometimes required to get parsing to work.\n",
    "val hackedPageXml = pageXml.replaceFirst(\n",
    "\"<text xml:space=\\\"preserve\\\" bytes=\\\"\\\\d+\\\">\",\n",
    "\"<text xml:space=\\\"preserve\\\">\")\n",
    "val page = new EnglishWikipediaPage()\n",
    "WikipediaPage.readPage(page, hackedPageXml)\n",
    "if (page.isEmpty) None\n",
    "else Some((page.getTitle, page.getContent))\n",
    "}\n",
    "val docTexts = rawXmls.filter(_ != null).flatMap(wikiXmlToPlainText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                  _1|                  _2|\n",
      "+--------------------+--------------------+\n",
      "|Category:Discrete...|Category:Discrete...|\n",
      "|Category:Incidenc...|Category:Incidenc...|\n",
      "|Category:Metric g...|Category:Metric g...|\n",
      "|Category:Integral...|Category:Integral...|\n",
      "|Category:Conforma...|Category:Conforma...|\n",
      "|Category:Trigonom...|Category:Trigonom...|\n",
      "|Category:Convex g...|Category:Convex g...|\n",
      "|             Coaxial|Coaxial\n",
      " \n",
      " \n",
      "\n",
      "In g...|\n",
      "|Category:Technica...|Category:Technica...|\n",
      "|   Category:Symmetry|Category:Symmetry...|\n",
      "|Category:Homogene...|Category:Homogene...|\n",
      "|       Ambient space|Ambient space\n",
      "An ...|\n",
      "|Category:Duality ...|Category:Duality ...|\n",
      "|          Superspace|Superspace\n",
      "Supers...|\n",
      "|     Geometry Center|Geometry Center\n",
      "T...|\n",
      "|          Dehn plane|Dehn plane\n",
      "In geo...|\n",
      "|Complex reflectio...|Complex reflectio...|\n",
      "|Category:Geometri...|Category:Geometri...|\n",
      "|    Lipschitz domain|Lipschitz domain\n",
      "...|\n",
      "|         Visual hull|Visual hull\n",
      " \n",
      "\n",
      "A ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTexts.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createNLPPipeline: ()edu.stanford.nlp.pipeline.StanfordCoreNLP\n",
       "isOnlyLetters: (str: String)Boolean\n",
       "plainTextToLemmas: (text: String, stopWords: Set[String], pipeline: edu.stanford.nlp.pipeline.StanfordCoreNLP)Seq[String]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def createNLPPipeline(): StanfordCoreNLP = {\n",
    "    val props = new Properties()\n",
    "    props.put(\"annotators\",\"tokenize, ssplit, pos, lemma\")\n",
    "    new StanfordCoreNLP(props)\n",
    "}\n",
    "\n",
    "def isOnlyLetters(str: String): Boolean = {\n",
    "    str.forall(c => Character.isLetter(c))\n",
    "}\n",
    "\n",
    "def plainTextToLemmas(text: String, stopWords: Set[String],\n",
    "    pipeline: StanfordCoreNLP): Seq[String] = {\n",
    "    val doc = new Annotation(text)\n",
    "    pipeline.annotate(doc)\n",
    "    \n",
    "    val lemmas = new ArrayBuffer[String]()\n",
    "    val sentences = doc.get(classOf[SentencesAnnotation])\n",
    "    for (sentence <- sentences.asScala;\n",
    "        token <- sentence.get(classOf[TokensAnnotation]).asScala){\n",
    "        val lemma = token.get(classOf[LemmaAnnotation])\n",
    "        if(lemma.length > 2 && !stopWords.contains(lemma)\n",
    "          && isOnlyLetters(lemma)){\n",
    "          lemmas += lemma.toLowerCase\n",
    "        }\n",
    "    }\n",
    "    lemmas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords = Set(down, it's, ourselves, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, with, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Set(down, it's, ourselves, that's, for, further, she'll, any, there's, this, haven't, in, ought, myself, have, your, off, once, i'll, are, is, his, why, too, why's, am, than, isn't, didn't, himself, but, you're, below, what, would, i'd, if, you'll, own, they'll, up, we're, they'd, so, our, do, all, him, had, nor, before, it, a, she's, as, hadn't, because, has, she, yours, or, above, yourself, herself, she'd, such, they, each, can't, don't, i, until, that, out, he's, cannot, to, we've, hers, you, did, let's, most, here, these, hasn't, was, there, when's, shan't, doing, at, through, been, over, i've, on, being, same, how, whom, my, after, who, itself, me, them, by, then, couldn't, he, should, few, wasn't, again, while, their, not, with, from, you've, they've, what's, wouldn't, both, could, its, under, which, you'd, an, be, here's, into, where, he'll, her, themselves, were, more, we'd, where's, they're, who's, between, aren't, ours, about, doesn't, how's, against, during, no, very, we, having, mustn't, some, does, when, shouldn't, yourselves, he'd, other, of, weren't, and, won't, theirs, i'm, we'll, the, those, only)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = scala.io.Source.fromFile(\"stopwords.txt\").getLines().toSet\n",
    "val bStopWords = spark.sparkContext.broadcast(stopWords)\n",
    "\n",
    "val terms: Dataset[(String, Seq[String])] = \n",
    " docTexts.mapPartitions { iter =>\n",
    "     val pipeline = createNLPPipeline()\n",
    "     iter.map { case(title, contents) =>\n",
    "        (title, plainTextToLemmas(contents, bStopWords.value, pipeline))\n",
    "     }\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termsDF = [title: string, terms: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[title: string, terms: array<string>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termsDF = terms.toDF(\"title\",\"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filtered = [title: string, terms: array<string>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[title: string, terms: array<string>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filtered = termsDF.where(size($\"terms\")>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "Adding annotator ssplit\n",
      "edu.stanford.nlp.pipeline.AnnotatorImplementations:\n",
      "Adding annotator pos\n",
      "Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [3.4 sec].\n",
      "Adding annotator lemma\n",
      "+--------------------+--------------------+\n",
      "|               title|               terms|\n",
      "+--------------------+--------------------+\n",
      "|Category:Discrete...|[category, discre...|\n",
      "|Category:Incidenc...|[category, incide...|\n",
      "|Category:Metric g...|[category, metric...|\n",
      "|Category:Integral...|[category, integr...|\n",
      "|Category:Conforma...|[category, confor...|\n",
      "|Category:Trigonom...|[category, trigon...|\n",
      "|Category:Convex g...|[category, convex...|\n",
      "|             Coaxial|[coaxial, geometr...|\n",
      "|Category:Technica...|[category, techni...|\n",
      "|   Category:Symmetry|[category, symmetry]|\n",
      "|Category:Homogene...|[category, homoge...|\n",
      "|       Ambient space|[ambient, space, ...|\n",
      "|Category:Duality ...|[category, dualit...|\n",
      "|          Superspace|[superspace, supe...|\n",
      "|     Geometry Center|[geometry, center...|\n",
      "|          Dehn plane|[dehn, plane, geo...|\n",
      "|Complex reflectio...|[complex, reflect...|\n",
      "|Category:Geometri...|[category, geomet...|\n",
      "|    Lipschitz domain|[lipschitz, domai...|\n",
      "|         Visual hull|[visual, hull, vi...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numTerms = 20000\n",
       "countVectorizer = cntVec_024dfea34f89\n",
       "vocabModel = cntVec_024dfea34f89\n",
       "docTermFreqs = [title: string, terms: array<string> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[title: string, terms: array<string> ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numTerms = 20000\n",
    "val countVectorizer = new CountVectorizer().\n",
    "    setInputCol(\"terms\").\n",
    "    setOutputCol(\"termFreqs\").\n",
    "    setVocabSize(numTerms)\n",
    "val vocabModel = countVectorizer.fit(filtered)\n",
    "val docTermFreqs = vocabModel.transform(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[title: string, terms: array<string> ... 1 more field]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTermFreqs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding annotator tokenize\n",
      "Adding annotator ssplit\n",
      "Adding annotator pos\n",
      "Adding annotator lemma\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`tfidVec`' given input columns: [title, terms, termFreqs, tfidfVec];;\n",
       "'Project [title#38, 'tfidVec]\n",
       "+- AnalysisBarrier\n",
       "      +- Project [title#38, terms#39, termFreqs#59, UDF(termFreqs#59) AS tfidfVec#95]\n",
       "         +- Project [title#38, terms#39, UDF(terms#39) AS termFreqs#59]\n",
       "            +- Filter (size(terms#39) > 1)\n",
       "               +- Project [_1#36 AS title#38, _2#37 AS terms#39]\n",
       "                  +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#36, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class java.lang.String), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class java.lang.String), true), true, false), assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, None) AS _2#37]\n",
       "                     +- MapPartitions <function1>, obj#35: scala.Tuple2\n",
       "                        +- DeserializeToObject newInstance(class scala.Tuple2), obj#34: scala.Tuple2\n",
       "                           +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#22, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#23]\n",
       "                              +- MapPartitions <function1>, obj#21: scala.Tuple2\n",
       "                                 +- DeserializeToObject cast(value#10 as string).toString, obj#20: java.lang.String\n",
       "                                    +- TypedFilter <function1>, class java.lang.String, [StructField(value,StringType,true)], cast(value#10 as string).toString\n",
       "                                       +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#10]\n",
       "                                          +- ExternalRDD [obj#9]\n",
       "\n",
       "StackTrace: 'Project [title#38, 'tfidVec]\n",
       "+- AnalysisBarrier\n",
       "      +- Project [title#38, terms#39, termFreqs#59, UDF(termFreqs#59) AS tfidfVec#95]\n",
       "         +- Project [title#38, terms#39, UDF(terms#39) AS termFreqs#59]\n",
       "            +- Filter (size(terms#39) > 1)\n",
       "               +- Project [_1#36 AS title#38, _2#37 AS terms#39]\n",
       "                  +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#36, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class java.lang.String), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class java.lang.String), true), true, false), assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, None) AS _2#37]\n",
       "                     +- MapPartitions <function1>, obj#35: scala.Tuple2\n",
       "                        +- DeserializeToObject newInstance(class scala.Tuple2), obj#34: scala.Tuple2\n",
       "                           +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#22, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#23]\n",
       "                              +- MapPartitions <function1>, obj#21: scala.Tuple2\n",
       "                                 +- DeserializeToObject cast(value#10 as string).toString, obj#20: java.lang.String\n",
       "                                    +- TypedFilter <function1>, class java.lang.String, [StructField(value,StringType,true)], cast(value#10 as string).toString\n",
       "                                       +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#10]\n",
       "                                          +- ExternalRDD [obj#9]\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3296)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1307)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1325)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF().\n",
    "    setInputCol(\"termFreqs\").\n",
    "    setOutputCol(\"tfidfVec\")\n",
    "val idfModel = idf.fit(docTermFreqs)\n",
    "val docTermMatrix = idfModel.transform(docTermFreqs).select(\"title\",\"tfidVec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
